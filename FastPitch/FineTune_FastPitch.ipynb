{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00568177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download local version of NeMo scripts. If you are running locally and want to use your own local NeMo code,\n",
    "# # comment out the below lines and set `code_dir` to your local path.\n",
    "code_dir = 'NeMoTTS'\n",
    "!git clone https://github.com/NVIDIA/NeMo.git {code_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53541090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from textgrid import TextGrid\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109edef",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53811d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 22050\n",
    "# Store all manifest and audios\n",
    "data_dir = 'dataset/EmoV-DB_sr22'\n",
    "# Store all supplementary files\n",
    "supp_dir = \"NeMoTTS_sup_data\"\n",
    "# Store all training logs\n",
    "logs_dir = \"NeMoTTS_logs\"\n",
    "# Store all mel-spectrograms for vocoder training\n",
    "mels_dir = \"NeMoTTS_mels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "data_dir = os.path.abspath(data_dir)\n",
    "os.makedirs(supp_dir, exist_ok=True)\n",
    "supp_dir = os.path.abspath(supp_dir)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "logs_dir = os.path.abspath(logs_dir)\n",
    "os.makedirs(mels_dir, exist_ok=True)\n",
    "mels_dir = os.path.abspath(mels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "    for entry in tqdm(data):\n",
    "        entry[\"text\"], entry[\"emotion\"] = entry[\"text\"].split(\"[EMOTION]\")[0].strip(),  entry[\"text\"].split(\"[EMOTION]\")[-1].strip()\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_path = \"/home/alina/datasets/EMOV-DB/EMOV/train_data.jsonl\"\n",
    "test_path = \"/home/alina/datasets/EMOV-DB/EMOV/test_data.jsonl\"\n",
    "\n",
    "train_data, valid_data = read_jsonl(train_path), read_jsonl(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath(data_dir)\n",
    "\n",
    "def add_full_path(example):\n",
    "    if example[\"emotion\"] == \"nuetral\": # Chage to Targe emotion\n",
    "        if example[\"emotion\"] == \"neutral\" or example[\"emotion\"] == \"anger\" :\n",
    "            example[\"audio_path\"] = os.path.join(data_dir,\"2\",example[\"emotion\"],\"audio\", example[\"audio_path\"])\n",
    "        elif example[\"emotion\"] == \"amused\" or example[\"emotion\"] == \"sleepiness\":\n",
    "            example[\"audio_path\"] = os.path.join(data_dir,\"1\",example[\"emotion\"],\"audio\", example[\"audio_path\"])\n",
    "        return example\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "train_data = [add_full_path(entry) for entry in train_data]\n",
    "valid_data = [add_full_path(entry) for entry in valid_data]\n",
    "train_data = [entry for entry in train_data if entry is not None]\n",
    "valid_data = [entry for entry in valid_data if entry is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_entry(entry):\n",
    "    # base_path='/home/alina/datasets/EMOV-DB/EMOV/'\n",
    "    del entry['target']\n",
    "    entry['audio_filepath'] = entry['audio_path']\n",
    "    y, sr = librosa.load(entry['audio_path'], sr=sample_rate)\n",
    "    entry['duration'] = librosa.get_duration(y=y, sr=sr)\n",
    "    return entry\n",
    "\n",
    "train_data = Parallel(n_jobs=-1)(\n",
    "    delayed(process_entry)(entry) for entry in tqdm(train_data, desc=\"Processing train split..\")\n",
    ")\n",
    "\n",
    "valid_data = Parallel(n_jobs=-1)(\n",
    "    delayed(process_entry)(entry) for entry in tqdm(valid_data, desc=\"Processing val split..\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5748b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_manifest(data, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest=os.path.join(data_dir, \"train_manifest.json\")\n",
    "val_manifest=os.path.join(data_dir, \"val_manifest.json\")\n",
    "save_manifest(train_data, train_manifest)\n",
    "save_manifest(valid_data, val_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880439f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {code_dir} && python scripts/dataset_processing/tts/extract_sup_data.py \\\n",
    "      manifest_filepath={train_manifest} \\\n",
    "      sup_data_path={supp_dir} \\\n",
    "      +sup_data_types='[\"pitch\", \"align_prior_matrix\"]' \\\n",
    "      +overwrite_sup_data=True \\\n",
    "      dataset.sample_rate={sample_rate} \\\n",
    "      dataset.n_fft=1024 \\\n",
    "      dataset.win_length=1024 \\\n",
    "      dataset.hop_length=256 \\\n",
    "      +dataloader_params.num_workers=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fee5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angry\n",
    "PITCH_MEAN=274.6726989746094\n",
    "PITCH_STD=73.51393127441406\n",
    "PITCH_MIN=107.0\n",
    "PITCH_MAX=566.0\n",
    "# Amused\n",
    "PITCH_MEAN=257.8951416015625\n",
    "PITCH_STD=74.55020904541016\n",
    "PITCH_MIN=86.0\n",
    "PITCH_MAX=569.0\n",
    "# Sleepiness \n",
    "PITCH_MEAN=330.51446533203125\n",
    "PITCH_STD=98.4043960571289\n",
    "PITCH_MIN=94.0\n",
    "PITCH_MAX=714.0\n",
    "# Neutral \n",
    "PITCH_MEAN=221.4080047607422\n",
    "PITCH_STD=65.20470428466797\n",
    "PITCH_MIN=59.0\n",
    "PITCH_MAX=442.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_dict_path = os.path.abspath(os.path.join(code_dir, \"scripts\", \"tts_dataset_files\", \"cmudict-0.7b_nv22.10\"))\n",
    "heteronyms_path = os.path.abspath(os.path.join(code_dir, \"scripts\", \"tts_dataset_files\", \"heteronyms-052722\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {code_dir} && python examples/tts/fastpitch_finetune.py \\\n",
    "  --config-name=\"fastpitch_align_v1.05.yaml\" \\\n",
    "  train_dataset={train_manifest} \\\n",
    "  validation_datasets={val_manifest} \\\n",
    "  sup_data_path={supp_dir} \\\n",
    "  phoneme_dict_path={phoneme_dict_path}\\\n",
    "  heteronyms_path={heteronyms_path} \\\n",
    "  name=\"exp0032_Neutral_st_4000_lr_2e_4_batch_32_align\" \\\n",
    "  exp_manager.exp_dir={logs_dir} \\\n",
    "  +init_from_pretrained_model=\"tts_en_fastpitch\" \\\n",
    "  +trainer.max_steps=4000 ~trainer.max_epochs \\\n",
    "  trainer.check_val_every_n_epoch=25 \\\n",
    "  ~model.optim.sched \\\n",
    "  model.train_ds.dataloader_params.batch_size=8 \\\n",
    "  ++trainer.accumulate_grad_batches=4 \\\n",
    "  model.validation_ds.dataloader_params.batch_size=24 \\\n",
    "  model.n_speakers=1 \\\n",
    "  model.pitch_mean={PITCH_MEAN} \\\n",
    "  model.pitch_std={PITCH_STD} \\\n",
    "  model.pitch_fmin={PITCH_MIN} \\\n",
    "  model.pitch_fmax={PITCH_MAX} \\\n",
    "  model.optim.lr=2e-4 \\\n",
    "  model.optim.name=adam \\\n",
    "  trainer.devices=1 \\\n",
    "  trainer.strategy=auto \\\n",
    "  +model.text_tokenizer.add_blank_at=True \\\n",
    "  ++exp_manager.create_tensorboard_logger=True \\\n",
    "  ++exp_manager.create_wandb_logger=False \\\n",
    "  +exp_manager.checkpoint_callback_params.save_top_k=1 \\\n",
    "  ++exp_manager.checkpoint_callback_params.monitor=val_loss \\\n",
    "  +exp_manager.checkpoint_callback_params.mode=min \\\n",
    "  +exp_manager.checkpoint_callback_params.save_last=True \\\n",
    "  trainer.check_val_every_n_epoch=5 \\\n",
    "  trainer.log_every_n_steps=5 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f48a92",
   "metadata": {},
   "source": [
    "### Inference with HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e229243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from collections import defaultdict\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeea464",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"exp0032_Neutral_st_4000_lr_2e_4_batch_32_align\"\n",
    "last_checkpoint_dir = sorted([i for i in (Path(logs_dir) / experiment_name).iterdir() if i.is_dir()])[-1] / \"checkpoints\"\n",
    "\n",
    "best_fastpitch_checkpoint = sorted(last_checkpoint_dir.glob(\"*val_loss=*.ckpt\"), key=lambda p: float(p.stem.split(\"val_loss=\")[1].split(\"-\")[0]) )[0]\n",
    "\n",
    "model = FastPitchModel.load_from_checkpoint(str(best_fastpitch_checkpoint))\n",
    "\n",
    "nemo_output_path = Path(logs_dir) / experiment_name / \"Neutral_FastPitch_best.nemo\"\n",
    "model.save_to(str(nemo_output_path))\n",
    "\n",
    "print(f\"Best checkpoint used: {best_fastpitch_checkpoint}\")\n",
    "print(f\"Model saved to: {nemo_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78262ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)\n",
    "spec_model = FastPitchModel.restore_from(nemo_output_path).eval().cuda()\n",
    "vocoder_model = HifiGanModel.from_pretrained(\"tts_en_hifigan\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_sample(\n",
    "    text: str,\n",
    "    speaker_id: int,\n",
    "    spec_gen_model,\n",
    "    vocoder_model,\n",
    "    sample_rate: int,\n",
    "    output_dir: str,\n",
    "    base_name: str,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    tokens = spec_gen_model.parse(text)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(\n",
    "            tokens=tokens,\n",
    "            speaker=None,\n",
    "            reference_spec=None,\n",
    "            reference_spec_lens=None\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "    audio = audio.squeeze().cpu().numpy().astype(np.float32)\n",
    "    if np.max(np.abs(audio)) > 1.0:\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "    wav_path = os.path.join(output_dir, f\"{base_name}_gen.wav\")\n",
    "    sf.write(wav_path, audio, samplerate=sample_rate, format='WAV', subtype='PCM_16')\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{base_name}_text.txt\"), \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spectrogram[0].cpu().numpy(), origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Spectrogram for {base_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{base_name}_spec.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8285aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val_record in tqdm(enumerate(valid_data)):\n",
    "    if val_record[\"emotion\"] == \"neutral\":\n",
    "        generate_and_save_sample(\n",
    "            text= val_record['text'],\n",
    "            speaker_id=0,\n",
    "            spec_gen_model=spec_model,\n",
    "            vocoder_model=vocoder_model,\n",
    "            sample_rate=22050,\n",
    "            output_dir=f\"{logs_dir}/{experiment_name}/eval\",\n",
    "            base_name=f\"{val_record['audio_path'].split('/')[-1].replace('.wav', '')}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
