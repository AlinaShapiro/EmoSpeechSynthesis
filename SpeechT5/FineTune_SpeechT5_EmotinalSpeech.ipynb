{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor\n",
    "from transformers import SpeechT5ForTextToSpeech\n",
    "from transformers import SpeechT5HifiGan\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import manual_seed\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from textgrid import TextGrid\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from speechbrain.inference import EncoderClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8543cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed(32)\n",
    "class Config:\n",
    "    device='cuda'if torch.cuda.is_available() else 'cpu'\n",
    "    sampling_rate=16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10be35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model=model.to(Config.device)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\") \n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[0][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "def infer(text,emotion,file_name):\n",
    "    combined_input = f\"{text} [EMOTION] {emotion}\"\n",
    "    inputs = processor(text=combined_input, return_tensors=\"pt\")\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "    sf.write(f\"{file_name}.wav\", speech.numpy(), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14296/14296 [00:43<00:00, 329.45it/s] \n"
     ]
    }
   ],
   "source": [
    "### One Speaker - One emotion\n",
    "ds_root = \"/home/alina/datasets/EMOV-DB/EMOV\"\n",
    "emo_abv = {'amused': '1', 'sleepiness': '1', 'anger': '2', 'neutral': '2'}\n",
    "\n",
    "def process_file(emotion, file, path):\n",
    "    try:\n",
    "        if not file.endswith(\".wav\") or emotion not in file:\n",
    "            return None\n",
    "        \n",
    "        textgrid_path = os.path.join(path, file.replace(\".wav\", \".TextGrid\"))\n",
    "        tg = TextGrid.fromFile(textgrid_path)\n",
    "        words_tier = tg.getFirst(\"words\")\n",
    "        transcription = [interval.mark for interval in words_tier if interval.mark.strip()]\n",
    "        text = \" \".join(transcription)\n",
    "        \n",
    "        wav_path = os.path.join(path, file)\n",
    "        waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "        if sr != 16000:\n",
    "            waveform = T.Resample(orig_freq=sr, new_freq=16000)(waveform)\n",
    "\n",
    "        input_text = f\"{text} [EMOTION] {file.split('_')[0]}\"\n",
    "        audio_path = file\n",
    "        return input_text, waveform, emotion, audio_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "tasks = []\n",
    "for emotion, subfolder in emo_abv.items():\n",
    "    path = os.path.join(ds_root, subfolder)\n",
    "    for file in os.listdir(path):\n",
    "        tasks.append((emotion, file, path))\n",
    "\n",
    "results = Parallel(n_jobs=1)(delayed(process_file)(emotion, file, path) for emotion, file, path in tqdm(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7942569",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, waveforms, emotions, audio_paths = zip(*[r for r in results if r is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09742c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts), len(waveforms), len(emotions), len(audio_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\n",
    "    'text': texts,\n",
    "    'target': waveforms,\n",
    "    'emotion': emotions,\n",
    "    'audio_path': audio_paths\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9023dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Two speakers - one emotion\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import os\n",
    "from textgrid import TextGrid\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "ds_root = \"/home/alina/datasets/EMOV-DB/EMOV\"\n",
    "emo_abv = {'amused': '1', 'sleepiness': '1', 'anger': '2', 'neutral': '2'}\n",
    "\n",
    "resampler = T.Resample(orig_freq=48000, new_freq=16000)\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        if not file_path.endswith(\".wav\"):\n",
    "            return None\n",
    "\n",
    "        filename = os.path.basename(file_path)\n",
    "        emotion = None\n",
    "        for emo in emo_abv:\n",
    "            if filename.startswith(emo):\n",
    "                emotion = emo\n",
    "                break\n",
    "        if emotion is None:\n",
    "            return None\n",
    "\n",
    "        textgrid_path = file_path.replace(\".wav\", \".TextGrid\")\n",
    "        tg = TextGrid.fromFile(textgrid_path)\n",
    "        words_tier = tg.getFirst(\"words\")\n",
    "        transcription = [interval.mark for interval in words_tier if interval.mark.strip()]\n",
    "        text = \" \".join(transcription)\n",
    "\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if sr != 16000:\n",
    "            waveform = T.Resample(orig_freq=sr, new_freq=16000)(waveform)\n",
    "\n",
    "        input_text = f\"{text} [EMOTION] {emotion}\"\n",
    "        audio_path = os.path.relpath(file_path, ds_root)\n",
    "\n",
    "        return {\n",
    "            \"text\": input_text,\n",
    "            \"target\": waveform,\n",
    "            \"emotion\": emotion,\n",
    "            \"audio_path\": audio_path\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "all_files = []\n",
    "for subfolder in ['1', '2']:\n",
    "    folder_path = os.path.join(ds_root, subfolder)\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            all_files.append(os.path.join(folder_path, file))\n",
    "\n",
    "results = Parallel(n_jobs=4)(\n",
    "    delayed(process_file)(file_path) for file_path in tqdm(all_files)\n",
    ")\n",
    "\n",
    "processed_data = [r for r in results if r is not None]\n",
    "\n",
    "dataset = Dataset.from_list(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1c6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|██████████| 1209/1209 [03:48<00:00,  5.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name, \n",
    "    run_opts={\"device\": Config.device}, \n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name)\n",
    ")\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings\n",
    "\n",
    "def prepare_dataset(single_data):\n",
    "    example = processor(\n",
    "        text=single_data[\"text\"],\n",
    "        audio_target=single_data['target'], \n",
    "        sampling_rate=Config.sampling_rate,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "    example[\"emotion\"] = single_data[\"emotion\"]\n",
    "    example[\"text\"] = single_data[\"text\"]\n",
    "    example[\"audio_path\"] = single_data[\"audio_path\"]\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(single_data['target'])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names, desc=\"Processing dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e32473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering too long samples...: 100%|██████████| 1209/1209 [00:00<00:00, 34658.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def is_not_too_long(input_ids):\n",
    "    \n",
    "    input_length = len(input_ids)\n",
    "    return input_length < 200\n",
    "\n",
    "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"],  desc=\"Filtering too long samples...\")\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For One Speaker - One Emotion\n",
    "dataset[\"train\"].to_json(\"/home/alina/datasets/EMOV-DB/EMOV/train_data.jsonl\", lines=True)\n",
    "dataset[\"test\"].to_json(\"/home/alina/datasets/EMOV-DB/EMOV/test_data.jsonl\", lines=True)\n",
    "\n",
    "# For Two Speakers - One Emotion\n",
    "dataset[\"train\"].to_json(\"/home/alina/datasets/EMOV-DB/EMOV/train_data_double.jsonl\", lines=True)\n",
    "dataset[\"test\"].to_json(\"/home/alina/datasets/EMOV-DB/EMOV/test_data_double.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fbb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/alina/datasets/EMOV-DB/EMOV/1\"\n",
    "\n",
    "def add_full_path(example):\n",
    "    example[\"audio_path\"] = os.path.join(base_path, example[\"audio_path\"])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9931b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples: 1088\n",
      "Total test samples: 121\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total train samples: {len(dataset['train'])}\\nTotal test samples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "        emotions = [feature[\"emotion\"] for feature in features]\n",
    "\n",
    "        batch = processor.pad(\n",
    "            input_ids=input_ids,\n",
    "            labels=label_features,\n",
    "            return_tensors=\"pt\")        \n",
    "\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100)\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor([len(feature[\"input_values\"]) for feature in label_features])\n",
    "            target_lengths = target_lengths.new([length - length % model.config.reduction_factor for length in target_lengths])\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "        batch[\"emotion\"] = emotions\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "data_collator = TTSDataCollatorWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.0.dev0\n",
      "/home/ashapiro/miniconda3/envs/t5/lib/python3.10/site-packages/transformers/__init__.py\n",
      "transformers.training_args_seq2seq\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/alina/repos/SpeechT5/experiments/exp0045\", \n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    eval_steps=100,\n",
    "    logging_steps=5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=5,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab43421",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bf2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt='/home/alina/repos/SpeechT5/experiments/exp0045/checkpoint-2000'\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(ckpt)\n",
    "model=model.to(Config.device)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\") \n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[0][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "def infer(text,emotion,file_name):\n",
    "    combined_input = f\"{text} [EMOTION] {emotion}\"\n",
    "    inputs = processor(text=combined_input, return_tensors=\"pt\")\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "    sf.write(f\"{file_name}.wav\", speech.numpy(), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1325c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "\n",
    "def gen(prompt, emo, model, speaker_embeddings, save_dir=\"outputs\", save_name=\"output\", device=\"cpu\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    text = f\"{prompt} [EMOTION] {emo}\"\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    model.to(device)\n",
    "    vocoder.to(device)\n",
    "    spectrogram = model.generate_speech(inputs[\"input_ids\"].to(device), speaker_embeddings.to(device))\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        speech = vocoder(spectrogram)\n",
    "\n",
    "    wav_path = os.path.join(save_dir, f\"{os.path.basename(save_name)}\")\n",
    "    torchaudio.save(wav_path, speech.unsqueeze(0).cpu(), 16000)\n",
    "\n",
    "    spec_path = os.path.join(save_dir, f\"{save_name.replace('.wav', '_spec.pt')}\")\n",
    "    torch.save(spectrogram.cpu(), spec_path)\n",
    "\n",
    "    print(f\"Saved wav to {wav_path}\")\n",
    "    print(f\"Saved spectrogram to {spec_path}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/alina/repos/SpeechT5/experiments/exp0045/evaluation\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for i, example in tqdm(enumerate(dataset[\"test\"])):\n",
    "    speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n",
    "    emotion = example[\"emotion\"]\n",
    "    if emotion != \"sleepiness\":\n",
    "        continue\n",
    "    text=example[\"text\"]\n",
    "    save_name=example[\"audio_path\"].split(\"/\")[-1]\n",
    "\n",
    "    gen(prompt=text,\n",
    "        emo=emotion,\n",
    "        model=model,\n",
    "        speaker_embeddings=speaker_embeddings,\n",
    "        save_dir=save_dir,\n",
    "        save_name=save_name,\n",
    "        device=device)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
